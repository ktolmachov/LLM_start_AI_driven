## Технологии

- Язык программирования: Python
- Интеграция с Telegram: aiogram (асинхронная работа с Telegram Bot API)
- Вызов LLM: через openrouter, используя совместимый openai client
- Хранение истории диалога: простые структуры Python (dict/list), без базы данных
- Тестирование: pytest
- Управление зависимостями: uv
- Автоматизация сборки, запуска и деплоя: make
- Деплой: Docker 

## Принцип разработки

- Минимум кода, только необходимый функционал для проверки идеи.
- Разработка итерациями: сначала рабочий прототип, затем доработка по обратной связи.
- Простой и понятный код, без усложнений и абстракций.
- Архитектура строится на функциях, без ООП (functional-first).
- Все настройки — через переменные окружения или простой конфиг-файл.
- Документирование только ключевых моментов (README, vision.md).
- Покрытие базовых сценариев тестами (pytest). 

## Структура проекта

- main.py — точка входа, запуск бота
- bot.py — логика Telegram-бота (обработчики, маршрутизация)
- llm_client.py — функции для работы с LLM через openrouter
- config.py — функции для загрузки конфигурации/переменных окружения
- prompts/ — папка с промптами и справочной информацией для LLM
- tests/ — тесты (pytest)
- requirements.txt или pyproject.toml — зависимости
- Dockerfile, Makefile — для сборки и деплоя
- doc/ — документация (README.md, vision.md и др.) 

## Архитектура проекта

- Вся логика разделена на функции.
- main.py инициализирует конфиг, LLM-клиент и Telegram-бота.
- bot.py содержит обработчики сообщений и маршрутизацию команд.
- llm_client.py — функции для общения с LLM через openrouter.
- История диалога пользователя хранится в памяти (dict/list), привязана к user_id.
- В prompts/ лежит системный промпт и справочная информация для LLM.
- Нет базы данных, нет сложных слоёв абстракции.
- Все настройки — через config.py (переменные окружения или простой .env).
- Документация хранится в директории doc/. 

## Модель данных

- Нет сложной модели данных, всё максимально просто.
- История диалога: dict, где ключ — user_id, значение — list сообщений (каждое сообщение — dict с ролью и текстом).
- Пример:
  ```python
  history = {
      user_id1: [
          {"role": "system", "text": "[Системный промпт с инфо о компании]"},
          {"role": "user", "text": "Привет!"},
          {"role": "assistant", "text": "Здравствуйте! Чем могу помочь?"}
      ],
      user_id2: [
          ...
      ]
  }
  ```
- Вся информация о компании и услугах — в системном промпте (отдельный файл в prompts/).
- Временные данные (например, текущий статус диалога) — тоже в памяти, в простых структурах. 

## Работа с LLM

- Вся работа с LLM — через отдельный модуль (llm_client.py).
- Используется openrouter API через openai-совместимый клиент.
- В системном промпте (prompts/system.txt) содержится вся информация о компании и услугах.
- На каждый запрос пользователя формируется список сообщений (история + системный промпт), который отправляется в LLM.
- Ответ LLM возвращается пользователю и добавляется в историю.
- Нет сложных цепочек, только один вызов LLM на каждый пользовательский запрос. 

## Мониторинг LLM

- Логирование запросов и ответов LLM.

## Мониторинг LLM

- Базовое логирование всех запросов к LLM и ответов (без хранения персональных данных).
- Логирование ошибок и таймаутов при обращении к LLM.
- При необходимости — простая метрика: количество успешных/ошибочных запросов.
- На MVP — только стандартный logging, без внешних систем мониторинга. 

## Сценарии работы

- Пользователь пишет боту в Telegram.
- Бот добавляет сообщение пользователя в историю (dict/list).
- Формируется запрос к LLM (история + системный промпт).
- Ответ LLM отправляется пользователю и добавляется в историю.
- Возможность начать новый диалог (команда /start или /reset).
- Все данные — только в памяти, без сохранения между перезапусками. 

## Деплой

- Всё приложение упаковывается в Docker-контейнер.
- Для сборки и запуска используется Makefile (make build, make run).
- Все переменные (токены, ключи) — через переменные окружения.
- Деплой — вручную на своей машине через запуск Docker-контейнера.
- Нет CI/CD, нет оркестрации — только ручной запуск контейнера. 

## Подход к конфигурированию

- Все настройки (токены, ключи, параметры LLM) хранятся в .env файле.
- Для загрузки .env используется config.py.
- Нет сложных конфигов, всё максимально просто и явно. 

## Подход к логгированию

- Используется стандартный модуль logging из Python.
- Логируются только ключевые события: запросы и ответы LLM, ошибки.
- Только консольный вывод, без внешних систем и файлов. 